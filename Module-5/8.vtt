WEBVTT

1
00:00:00.000 --> 00:00:11.979
Welcome to Cross-Validation and Advanced Model Validation Techniques.

2
00:00:11.979 --> 00:00:15.479
After watching this video, you will be able to define model validation.

3
00:00:15.479 --> 00:00:19.079
You will also be able to explain what data snooping is and how to avoid it.

4
00:00:19.079 --> 00:00:22.690
Finally, you will be able to discuss key strategies for model validation.

5
00:00:22.690 --> 00:00:27.250
Model validation is all about doing your best to optimize your model without jeopardizing

6
00:00:27.250 --> 00:00:30.270
its ability to predict well on unseen data.

7
00:00:30.270 --> 00:00:36.080
It helps you prevent overfitting when selecting the best model configuration by tuning hyperparameters.

8
00:00:36.080 --> 00:00:41.759
Consider the basic train or test-split evaluation method, where the data set is split into two parts.

9
00:00:41.759 --> 00:00:45.759
A training set is used to train the model, and a test set is used to evaluate or estimate

10
00:00:45.759 --> 00:00:48.610
the model's ability to predict outcomes from unseen data.

11
00:00:48.610 --> 00:00:54.000
Most machine learning models have optional parameter settings, called model hyperparameters,

12
00:00:54.000 --> 00:00:57.130
that affect how well the model fits the data used to train it.

13
00:00:57.130 --> 00:01:00.230
What if you tried different hyperparameters for your model,

14
00:01:00.230 --> 00:01:03.159
and then chose the one that performed best on the testing data?

15
00:01:03.159 --> 00:01:07.440
Wouldn't you effectively fit the model to the testing data, not the training data?

16
00:01:07.440 --> 00:01:09.220
This would result in overfitting.

17
00:01:09.220 --> 00:01:13.080
Your model likely wouldn't generalize well to unseen data, invalidating it.

18
00:01:13.080 --> 00:01:17.360
Checking performance on the test data before you are done optimizing your model is called

19
00:01:17.360 --> 00:01:20.610
data snooping, a form of what's known as data leakage.

20
00:01:20.610 --> 00:01:25.650
What can you do to validate your model to ensure it doesn't overfit itself to your test data?

21
00:01:25.650 --> 00:01:28.910
You need to decouple model tuning from the final evaluation.

22
00:01:28.910 --> 00:01:33.460
Validation means tuning your model on the training data, but only testing it on unseen

23
00:01:33.460 --> 00:01:36.350
test data once you are satisfied that it is well trained.

24
00:01:36.350 --> 00:01:38.080
There is no snooping involved.

25
00:01:38.080 --> 00:01:43.160
Here is a model validation strategy that involves segmenting your data into at least three parts.

26
00:01:43.160 --> 00:01:48.530
A training set, which is used to train the model, including optimizing its hyperparameters.

27
00:01:48.530 --> 00:01:53.650
One or more validation sets or subsets of the training data used during the model optimization

28
00:01:53.650 --> 00:01:56.760
process to evaluate a machine learning model's performance.

29
00:01:56.760 --> 00:02:00.480
A test set that is held back, unseen data used for final evaluation

30
00:02:00.480 --> 00:02:03.060
after model training and validation.

31
00:02:03.060 --> 00:02:05.800
Cross-validation enables hyperparameter tuning.

32
00:02:05.800 --> 00:02:09.610
Here is the cross-validation algorithm for model tuning and validation.

33
00:02:09.610 --> 00:02:12.919
Split your data into training data and testing data.

34
00:02:12.919 --> 00:02:16.760
Further split your training data into a training set and a validation set.

35
00:02:16.760 --> 00:02:20.759
Optimize your model's hyperparameters by repeatedly training it on the training set

36
00:02:20.759 --> 00:02:23.330
and measuring its performance on the validation set.

37
00:02:23.330 --> 00:02:25.350
Choose your best set of hyperparameters

38
00:02:25.350 --> 00:02:29.680
and evaluate your resulting best model on your completely unseen testing data.

39
00:02:29.680 --> 00:02:35.320
You now have a validated model and an estimate of how well it will generalize on new unseen data.

40
00:02:35.320 --> 00:02:40.360
Some potential validation problems reside in selecting a single, specific validation set.

41
00:02:40.360 --> 00:02:43.740
Your model could be overfitting to this specific data set.

42
00:02:43.740 --> 00:02:47.979
If your model needs a lot of data to train on, you may not have enough data left over

43
00:02:47.979 --> 00:02:50.559
for validation and testing purposes.

44
00:02:50.559 --> 00:02:56.020
This means your training and validation data might not be representative of the sample population.

45
00:02:56.020 --> 00:03:00.000
Your model may not be learning details, like noise on this particular set.

46
00:03:00.000 --> 00:03:03.690
Your model's performance may not be stable across different validation sets.

47
00:03:03.690 --> 00:03:08.490
A solution to avoid overfitting your test data while trying to optimize the model's hyperparameters

48
00:03:08.490 --> 00:03:10.720
consists of a few key steps.

49
00:03:10.720 --> 00:03:15.820
Divide your data into K equal-sized folds to be used as validation subsets.

50
00:03:15.820 --> 00:03:19.520
For each trial model or set of hyperparameters, and for each fold,

51
00:03:19.520 --> 00:03:23.380
train a model on the remaining K minus 1 folds.

52
00:03:23.380 --> 00:03:26.970
Test the model on the selected fold and store this model's score.

53
00:03:26.970 --> 00:03:29.970
Compute an aggregated score of overall folds.

54
00:03:29.970 --> 00:03:33.220
Select the set of hyperparameters that led to the best model.

55
00:03:33.220 --> 00:03:37.070
Notice that every data point is used both for training and validation,

56
00:03:37.070 --> 00:03:40.660
greatly increasing the utilization of the data you have on hand.

57
00:03:40.690 --> 00:03:46.170
K-fold cross-validation, typically 5- to 10-fold, provides a more robust technique for estimating

58
00:03:46.170 --> 00:03:49.860
your model's generalizability to unseen real-world data.

59
00:03:49.860 --> 00:03:52.460
Varying the validation set has several benefits.

60
00:03:52.460 --> 00:03:56.020
It greatly increases the data on which the model trains and tests.

61
00:03:56.020 --> 00:04:00.300
It reduces overfitting because it smooths out unwanted details that are particular

62
00:04:00.300 --> 00:04:02.100
to a chosen training subset.

63
00:04:02.100 --> 00:04:07.770
Consequently, it improves your ability to evaluate how well your model will generalize to unseen data.

64
00:04:07.770 --> 00:04:11.720
In classification problems, you might have many observations in one class

65
00:04:11.720 --> 00:04:13.170
and very few in another.

66
00:04:13.170 --> 00:04:16.580
It means you are dealing with an imbalanced classification problem.

67
00:04:16.580 --> 00:04:22.220
Stratified cross-validation ensures that the class distribution is preserved in each validation fold,

68
00:04:22.220 --> 00:04:24.890
preventing bias in the evaluation process.

69
00:04:24.890 --> 00:04:30.000
The analog to imbalanced data in regression problems is when your target is highly skewed.

70
00:04:30.000 --> 00:04:33.100
Many models assume your target is normally distributed.

71
00:04:33.100 --> 00:04:39.560
Fortunately, you can transform your target variable using methods like log- or box-cox transforms

72
00:04:39.560 --> 00:04:43.170
to reduce the skewness and fit your model to the transformed target.

73
00:04:43.170 --> 00:04:47.770
Consider an example of a skewed target variable, as depicted by the histogram on the left.

74
00:04:47.770 --> 00:04:51.350
Lower target values have a much higher frequency than higher values.

75
00:04:51.350 --> 00:04:55.000
The other two histograms illustrate the distributions of box-cox

76
00:04:55.000 --> 00:04:57.770
and logarithmic transforms of the target data.

77
00:04:57.770 --> 00:05:00.640
Both transforms significantly reduce the skewness.

78
00:05:00.640 --> 00:05:05.940
Observe how well linear regression can fit each of these representations of the target variable.

79
00:05:05.940 --> 00:05:08.250
In this video, you learned:

80
00:05:08.250 --> 00:05:12.800
that model validation helps you prevent overfitting when selecting the best model configuration

81
00:05:12.800 --> 00:05:14.380
by tuning hyperparameters.

82
00:05:14.380 --> 00:05:20.010
Checking performance on the test data before you are done optimizing your model is called data snooping.

83
00:05:20.010 --> 00:05:25.400
Model validation involves dividing data into training set, validation set, and test set.

84
00:05:25.400 --> 00:05:28.417
Cross-validation enables hyperparameter tuning.

85
00:05:28.417 --> 00:05:30.940
A solution to avoid overfitting your test data

86
00:05:30.940 --> 00:05:35.180
while trying to optimize the model's hyperparameters is K-fold cross-validation.

87
00:05:35.180 --> 00:05:37.383
Stratified cross-validation ensures

88
00:05:37.383 --> 00:05:44.380
that the class distribution is preserved in each validation fold, preventing bias.