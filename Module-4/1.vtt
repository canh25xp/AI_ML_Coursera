WEBVTT

1
00:00:00.000 --> 00:00:11.640
Welcome to this video on clustering strategies in real-world applications.

2
00:00:11.640 --> 00:00:15.460
After watching this video, you will be able to explain the concept of clustering and its

3
00:00:15.460 --> 00:00:17.120
various applications.

4
00:00:17.120 --> 00:00:21.840
You will learn how to apply k-means clustering to segment customers based on their characteristics.

5
00:00:21.840 --> 00:00:26.280
Additionally, you will gain an understanding of the different types of clustering methods,

6
00:00:26.280 --> 00:00:30.159
including partition-based, density-based, and hierarchical clustering.

7
00:00:30.159 --> 00:00:35.700
Finally, you will be equipped to analyze agglomerative and divisive hierarchical clustering,

8
00:00:35.700 --> 00:00:39.040
exploring their distinct approaches to grouping data points.

9
00:00:39.040 --> 00:00:42.439
Clustering is a machine learning technique that automatically groups data points into

10
00:00:42.439 --> 00:00:44.799
clusters based on similarities.

11
00:00:44.799 --> 00:00:48.820
Clustering can be applied in various scenarios, such as identifying music genres,

12
00:00:48.820 --> 00:00:52.759
segmenting user groups, or analyzing market segments.

13
00:00:52.759 --> 00:00:58.779
This method can use just one feature or multiple features in data to form meaningful clusters.

14
00:00:58.779 --> 00:01:03.479
This dataset includes historical customer features and loan default status.

15
00:01:03.479 --> 00:01:08.480
Classification algorithms, being supervised, learn to predict categorical labels from labeled data.

16
00:01:08.480 --> 00:01:13.120
Here, based on the labeled historical data, a decision tree model is trained to predict

17
00:01:13.120 --> 00:01:16.219
if a new customer will default on a loan.

18
00:01:16.219 --> 00:01:19.239
Clustering is like classification but works with unlabeled data,

19
00:01:19.239 --> 00:01:22.120
independently finding patterns to form clusters.

20
00:01:22.180 --> 00:01:26.559
Here, a k-means clustering model segments customers with similar characteristics into

21
00:01:26.559 --> 00:01:31.160
three clusters, indicated by the blue, light blue, and red rows.

22
00:01:31.160 --> 00:01:35.519
The model operates without knowing if a customer has defaulted, as that data is unavailable

23
00:01:35.519 --> 00:01:37.440
and not part of its design.

24
00:01:37.440 --> 00:01:43.040
In exploratory data analysis, clustering uncovers natural groupings, such as customer segmentation,

25
00:01:43.040 --> 00:01:45.260
for targeted marketing.

26
00:01:45.260 --> 00:01:50.440
Clustering boosts pattern recognition by grouping similar objects and aiding in image segmentation,

27
00:01:50.440 --> 00:01:53.400
such as detecting medical abnormalities.

28
00:01:53.400 --> 00:01:59.440
Clustering helps anomaly detection by identifying outliers and detecting fraud or equipment malfunctions.

29
00:01:59.440 --> 00:02:03.780
In future engineering, clustering creates new features or reduces dimensionality,

30
00:02:03.780 --> 00:02:06.720
improving model performance and interpretability.

31
00:02:06.720 --> 00:02:10.079
In data summarization, clustering simplifies data by summarizing it

32
00:02:10.079 --> 00:02:13.199
into a small number of representative clusters.

33
00:02:13.199 --> 00:02:16.780
Clustering reduces data size by replacing data points with cluster centers,

34
00:02:16.780 --> 00:02:19.000
which is useful for image compression.

35
00:02:19.000 --> 00:02:24.240
Finally, clustering identifies essential features that distinguish clusters.

36
00:02:24.240 --> 00:02:28.399
Partition-based clustering algorithms divide data into non-overlapping groups.

37
00:02:28.399 --> 00:02:33.119
The most common method, k-means, identifies k-clusters with minimal variance.

38
00:02:33.119 --> 00:02:37.160
These algorithms are efficient and scale well with large datasets.

39
00:02:37.160 --> 00:02:41.079
Density-based clustering algorithms create clusters of any shape,

40
00:02:41.079 --> 00:02:44.279
making them suitable for irregular clusters and noisy datasets.

41
00:02:44.279 --> 00:02:47.559
An example is DBSCAN algorithm.

42
00:02:47.600 --> 00:02:52.060
Hierarchical clustering algorithms organize data into a tree of nested clusters,

43
00:02:52.060 --> 00:02:55.000
each containing smaller sub-clusters.

44
00:02:55.000 --> 00:02:59.720
This process generates a dendrogram, revealing relationships between clusters.

45
00:02:59.720 --> 00:03:03.479
The two main algorithms are agglomerative, which merges clusters,

46
00:03:03.479 --> 00:03:05.720
and divisive, which splits them.

47
00:03:05.720 --> 00:03:10.679
These algorithms are intuitive and effective for small to mid-sized datasets.

48
00:03:10.679 --> 00:03:16.039
This partition-based clustering result uses the makeBlobs function from Scikit-learn,

49
00:03:16.039 --> 00:03:19.360
generating three color-coded clusters in the scatterplot.

50
00:03:19.360 --> 00:03:23.360
Here are two clustering results using the makeMoons function from Scikit-learn

51
00:03:23.360 --> 00:03:25.880
that generates interlocking half-circles.

52
00:03:25.880 --> 00:03:28.720
Both use color to distinguish clusters visually.

53
00:03:28.720 --> 00:03:32.739
On the left, the partition-based clustering struggles to separate the shapes,

54
00:03:32.739 --> 00:03:35.520
partitioning the data along a red curve.

55
00:03:35.520 --> 00:03:39.660
In contrast, the density-based clustering successfully separates the shapes,

56
00:03:39.660 --> 00:03:43.600
but creates an unnecessary third cluster of three points.

57
00:03:43.639 --> 00:03:49.679
This chart, created by UCLA biologists, presents genetic data from over 900 dogs across

58
00:03:49.679 --> 00:03:52.639
85 breeds and 200 wild grey wolves globally.

59
00:03:52.639 --> 00:03:56.600
They analyzed 48,000 genetic markers using molecular techniques.

60
00:03:56.600 --> 00:04:01.520
The diagram illustrates hierarchical clustering, grouping animals based on genetic similarities

61
00:04:01.520 --> 00:04:07.000
in a tree-like structure, where each node represents a cluster of child clusters.

62
00:04:07.000 --> 00:04:13.399
There are two main strategies for constructing hierarchical clustering trees, divisive and agglomerative.

63
00:04:13.399 --> 00:04:15.759
Divisive clustering uses a top-down approach.

64
00:04:15.759 --> 00:04:18.859
It starts with all observations in a single root cluster,

65
00:04:18.859 --> 00:04:22.780
which is iteratively split into smaller child clusters.

66
00:04:22.780 --> 00:04:25.679
Agglomerative clustering employs a bottom-up approach.

67
00:04:25.679 --> 00:04:28.559
Each observation begins as an individual cluster,

68
00:04:28.559 --> 00:04:31.799
and similar clusters are merged into larger parent clusters.

69
00:04:31.799 --> 00:04:35.399
Let's explore the agglomerative hierarchical clustering algorithm,

70
00:04:35.399 --> 00:04:37.399
which uses a bottom-up approach.

71
00:04:37.399 --> 00:04:40.479
First, select a metric to measure the distance between clusters,

72
00:04:40.479 --> 00:04:43.320
such as the distance between their centroids.

73
00:04:43.320 --> 00:04:49.279
The process begins by initializing N clusters, with each cluster containing a single data point.

74
00:04:49.279 --> 00:04:54.959
Next, a distance matrix is computed, which is an n-x-n matrix that displays the distances

75
00:04:54.959 --> 00:04:58.279
d-i-j between each pair of points i and j.

76
00:04:58.279 --> 00:05:01.720
Repeat the following steps until you achieve the desired number of clusters,

77
00:05:01.720 --> 00:05:04.420
or merge all points into one cluster.

78
00:05:04.420 --> 00:05:08.559
Merge the two closest clusters based on the selected distance metric.

79
00:05:08.559 --> 00:05:11.399
Update the proximity metric with the new distance values.

80
00:05:12.000 --> 00:05:17.679
Let's examine the divisive hierarchical clustering algorithm, which takes a top-down approach.

81
00:05:17.679 --> 00:05:20.359
Start with the entire dataset as one cluster.

82
00:05:20.359 --> 00:05:25.320
Partition this cluster into smaller clusters based on similarities or dissimilarities.

83
00:05:25.320 --> 00:05:28.880
Continue splitting each cluster into two until a stopping criterion,

84
00:05:28.880 --> 00:05:31.920
which is a minimum cluster size, is reached.

85
00:05:31.920 --> 00:05:36.320
Imagine you want to group six cities in Canada based on their distances from one another.

86
00:05:36.320 --> 00:05:40.339
The distance matrix represents the distances between each pair of cities.

87
00:05:40.339 --> 00:05:44.660
The algorithm starts with six clusters, each representing a city with the first two letters

88
00:05:44.660 --> 00:05:46.000
of its name.

89
00:05:46.000 --> 00:05:51.179
The initial task is identifying which two clusters to merge based on a distance measure,

90
00:05:51.179 --> 00:05:52.700
like flight distance.

91
00:05:52.700 --> 00:05:57.179
Reviewing the distance matrix shows that Montreal and Ottawa are the closest clusters,

92
00:05:57.179 --> 00:06:00.519
so they combine into the next parent cluster.

93
00:06:00.519 --> 00:06:05.519
As the algorithm progresses, you can visualize the hierarchy of clusters with a dendrogram,

94
00:06:05.519 --> 00:06:07.440
as indicated by the red oval.

95
00:06:07.440 --> 00:06:11.819
The distance matrix combines the rows and columns for Montreal and Ottawa

96
00:06:11.819 --> 00:06:14.119
in the Ottawa-Montreal cluster.

97
00:06:14.119 --> 00:06:17.959
The distances to this new cluster are updated and calculated as the midpoint between the

98
00:06:17.959 --> 00:06:19.440
two cities.

99
00:06:19.440 --> 00:06:22.700
Next, look for the closest clusters again.

100
00:06:22.700 --> 00:06:26.000
The Ottawa-Montreal and Toronto clusters are the nearest,

101
00:06:26.000 --> 00:06:29.079
forming the Toronto-Ottawa-Montreal cluster.

102
00:06:29.079 --> 00:06:33.399
As the algorithm progresses, a dendrogram visualizes the hierarchy of clusters,

103
00:06:33.399 --> 00:06:35.160
as shown by the red oval.

104
00:06:35.160 --> 00:06:40.640
Next, the updated distance matrix shows that Vancouver and Edmonton are the closest.

105
00:06:40.640 --> 00:06:46.519
Similarly, the agglomerative algorithm merges clusters into one, completing the dendrogram.

106
00:06:46.519 --> 00:06:48.339
In this video, you learned to

107
00:06:48.339 --> 00:06:51.880
explain the concept of clustering and its applications,

108
00:06:51.880 --> 00:06:56.000
apply k-means clustering to segment customers based on characteristics.

109
00:06:56.000 --> 00:07:00.420
Explain density-based clustering and how they are suitable for irregular clusters.

110
00:07:00.420 --> 00:07:04.200
Explain hierarchical clustering and how it generates a dendrogram.

111
00:07:04.239 --> 00:07:08.359
Use strategies for hierarchical clustering, divisive and agglomerative.

112
00:07:08.359 --> 00:07:12.320
Analyze agglomerative hierarchical clustering and its bottom-up approach.

113
00:07:12.320 --> 00:07:15.320
Analyze divisive hierarchical clustering and its top-up approach.