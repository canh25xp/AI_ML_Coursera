WEBVTT

1
00:00:00.000 --> 00:00:11.340
Welcome to Regularization in Linear Regression.

2
00:00:11.340 --> 00:00:16.340
After watching this video, you will be able to define regularization for linear regression

3
00:00:16.340 --> 00:00:21.659
and compare linear, ridge, and lasso regression methods.

4
00:00:21.659 --> 00:00:24.620
Regularization is a regression technique to prevent overfitting.

5
00:00:24.620 --> 00:00:29.659
It constrains the model during training, discouraging it from overfitting to the training data.

6
00:00:29.659 --> 00:00:34.500
Regularization achieves this goal by suppressing the size of its coefficients.

7
00:00:34.500 --> 00:00:39.259
With regularization, a modified cost function is used to optimize the linear regression

8
00:00:39.259 --> 00:00:45.520
model, which has the general form, regularized cost function equals mean squared error plus

9
00:00:45.520 --> 00:00:48.099
lambda times penalty term.

10
00:00:48.099 --> 00:00:52.659
Here, lambda is a parameter that controls the influence of the penalty term, and the

11
00:00:52.659 --> 00:00:55.659
penalty measures the size of the coefficients.

12
00:00:55.659 --> 00:01:02.459
Common regularized regression methods like ridge and lasso regularization use specific penalty terms.

13
00:01:02.459 --> 00:01:06.379
Linear regression models the relationship between two or more variables by fitting a

14
00:01:06.379 --> 00:01:08.779
straight line to the given data set.

15
00:01:08.779 --> 00:01:13.379
In ordinary linear regression, predictions are a linear combination of features, and

16
00:01:13.379 --> 00:01:18.339
the goal is to minimize the loss function, usually measured as the mean squared error

17
00:01:18.339 --> 00:01:22.620
– MSE – between the predicted and actual target values.

18
00:01:22.620 --> 00:01:27.059
Mathematically, the linear regression model is defined by a linear combination of the

19
00:01:27.059 --> 00:01:37.699
form y-hat equals theta-zero plus theta-one x-one plus theta-two x-two plus theta-n x-n,

20
00:01:37.699 --> 00:01:42.379
where the x-i are the feature vectors that can be represented as a matrix X that includes

21
00:01:42.379 --> 00:01:48.900
a constant value of 1 in the first entry to account for the bias, or intercept term, theta-zero,

22
00:01:48.900 --> 00:01:53.660
and the thetas are the unknown weights, which can be represented as a matrix theta.

23
00:01:53.660 --> 00:01:58.260
The weights are commonly referred to as the coefficients of the linear regression model.

24
00:01:58.260 --> 00:02:04.139
Ridge and lasso are regularized forms of linear regression that differ only in their cost functions.

25
00:02:04.139 --> 00:02:09.500
Regular linear regression has no penalty term, while ridge regression uses an L2 or sum-of-squares

26
00:02:09.500 --> 00:02:13.580
penalty on its coefficients, which helps to shrink them.

27
00:02:13.580 --> 00:02:18.699
Lasso regression uses an L1 or sum-of-absolute-values penalty on its coefficients.

28
00:02:18.699 --> 00:02:22.899
This penalty can shrink some coefficients to exactly zero.

29
00:02:22.899 --> 00:02:27.339
Lasso regression responds well to feature sparsity, making it useful for feature selection

30
00:02:27.339 --> 00:02:29.699
and data compression tasks.

31
00:02:29.699 --> 00:02:34.639
In machine learning, sparse coefficients mean that only a small number of variables significantly

32
00:02:34.639 --> 00:02:39.020
contribute to a dataset, while the remaining have little or no impact.

33
00:02:39.020 --> 00:02:43.619
The plot here shows a simulated set of sparse coefficients, labeled as black dots, with

34
00:02:43.619 --> 00:02:46.740
a high signal-to-noise ratio, SNR.

35
00:02:46.779 --> 00:02:50.820
The simulation displays coefficient values for each of the 100 features.

36
00:02:50.820 --> 00:02:56.559
It has 5 non-sparse coefficients, with a high SNR meaning they stand out strongly.

37
00:02:56.559 --> 00:03:01.940
As you can see, all three regression methods predict the non-zero coefficients very well.

38
00:03:01.940 --> 00:03:06.380
Lasso finds the zero coefficients exactly, while linear and ridge have some difficulty

39
00:03:06.380 --> 00:03:10.660
predicting the zero coefficients, with linear regression doing slightly better than ridge

40
00:03:10.660 --> 00:03:12.000
regression.

41
00:03:12.000 --> 00:03:16.800
In this plot, we have a simulated set of sparse coefficients labeled as black dots, with a

42
00:03:16.800 --> 00:03:18.279
low SNR.

43
00:03:18.279 --> 00:03:22.979
Evidently, linear regression performs very poorly in this case, as it tends to greatly

44
00:03:22.979 --> 00:03:27.880
overestimate the ideal coefficients, overshoot most of the zero coefficients, and assign

45
00:03:27.880 --> 00:03:31.240
large negative coefficients when they should be zero.

46
00:03:31.240 --> 00:03:36.279
This illustrates the fact that ordinary linear regression is sensitive to noisy data.

47
00:03:36.279 --> 00:03:40.759
As you can also see, ridge and lasso have similar abilities in predicting the non-zero

48
00:03:40.759 --> 00:03:45.919
coefficients, but lasso is much better than ridge at finding the zero coefficients.

49
00:03:45.919 --> 00:03:50.800
Even in this low SNR environment, lasso is a great feature selector.

50
00:03:50.800 --> 00:03:54.479
This plot shows non-sparse coefficients with a high SNR.

51
00:03:54.479 --> 00:03:59.440
As you can see, all three regression methods predict the non-zero coefficients very well,

52
00:03:59.440 --> 00:04:02.699
with the ridge erring slightly more than the others.

53
00:04:02.699 --> 00:04:06.619
Lasso finds all of the zero coefficients, while linear and ridge have some difficulty

54
00:04:06.940 --> 00:04:12.179
the zero coefficients, with linear regression doing slightly better than ridge regression.

55
00:04:12.179 --> 00:04:16.339
In this plot, we have non-sparse coefficients with a low SNR.

56
00:04:16.339 --> 00:04:21.140
Evidently, linear regression performs very poorly in this case, as it tends to overestimate

57
00:04:21.140 --> 00:04:26.220
the ideal coefficients, overshoot most of the zero coefficients, and assign large negative

58
00:04:26.220 --> 00:04:29.940
coefficients even though all coefficients here are positive.

59
00:04:29.940 --> 00:04:34.519
This illustrates that ordinary linear regression is sensitive to noisy data.

60
00:04:34.519 --> 00:04:39.260
As you can also see, ridge regression slightly outperforms lasso when it comes to predicting

61
00:04:39.260 --> 00:04:44.260
the non-zero coefficients, but lasso is better at finding the zero coefficients.

62
00:04:44.260 --> 00:04:49.200
Even in this low SNR environment, lasso is a great feature selector.

63
00:04:49.200 --> 00:04:54.000
This chart displays the results of training lasso, ridge, and regular linear regression

64
00:04:54.000 --> 00:04:56.679
for a moderately noisy target variable.

65
00:04:56.679 --> 00:05:00.920
The results show the predictions made on the test data after each model was trained on

66
00:05:00.920 --> 00:05:02.959
70% of the dataset.

67
00:05:02.959 --> 00:05:08.200
The top row shows three scatter plots, comparing test predictions against the actual lasso,

68
00:05:08.200 --> 00:05:11.119
ridge, and regular regression test values.

69
00:05:11.119 --> 00:05:16.079
The result for lasso is much more concentrated around the ideal 45-degree line than the other

70
00:05:16.079 --> 00:05:17.519
two results.

71
00:05:17.519 --> 00:05:22.160
The bottom row of plots illustrates the exact comparisons instead of two superimposed plots

72
00:05:22.160 --> 00:05:25.160
of the regression predictions and actual values.

73
00:05:25.160 --> 00:05:30.079
Again, you can see that lasso outperformed both ridge and regular regression because

74
00:05:30.079 --> 00:05:33.160
its predictions track well with the actual values.

75
00:05:33.160 --> 00:05:37.119
In addition, the MSEs are displayed for all three models.

76
00:05:37.119 --> 00:05:41.940
The MSE for lasso is about 30 times less than the MSEs for ridge and regular linear

77
00:05:41.940 --> 00:05:43.559
regression.

78
00:05:43.559 --> 00:05:48.640
This table summarizes the relative performances of linear, ridge, and lasso regression methods

79
00:05:48.640 --> 00:05:53.399
in high and low SNR environments for sparse and non-sparse coefficients.

80
00:05:53.399 --> 00:05:57.600
In all scenarios, lasso performs the best out of the three methods.

81
00:05:57.600 --> 00:06:02.920
Regular linear regression performs well in high SNR environments and poorly in low SNR

82
00:06:02.920 --> 00:06:03.920
environments.

83
00:06:03.920 --> 00:06:07.959
The methods rank similarly whether the SNR is high or low, with the ridge holding its

84
00:06:07.959 --> 00:06:09.859
rating in all cases.

85
00:06:09.859 --> 00:06:13.040
For low SNR, lasso and ridge are clear winners.

86
00:06:13.040 --> 00:06:14.799
In this video, you learned

87
00:06:14.799 --> 00:06:18.959
Regularization is a regression technique to prevent overfitting.

88
00:06:18.959 --> 00:06:23.920
It constrains the model during training, discouraging it from overfitting to the training data.

89
00:06:23.920 --> 00:06:28.200
In ordinary linear regression, predictions are a linear combination of features, and

90
00:06:28.200 --> 00:06:30.640
the goal is to minimize the loss function.

91
00:06:30.640 --> 00:06:35.119
Ridge and lasso are regularized forms of linear regression that differ only in their cost

92
00:06:35.119 --> 00:06:36.739
functions.

93
00:06:36.739 --> 00:06:41.299
Regular linear regression has no penalty term, while ridge regression uses an L2, or sum

94
00:06:41.299 --> 00:06:45.480
of squares penalty, on its coefficients, which helps to shrink them.

95
00:06:45.480 --> 00:06:50.500
Lasso regressions use an L1, or sum of absolute values penalty on its coefficients.

96
00:06:50.500 --> 00:06:54.339
Linear regression suffers from overfitting in the presence of noise because it is highly

97
00:06:54.339 --> 00:06:56.220
sensitive to outliers.

98
00:06:56.220 --> 00:07:01.179
You can use regularization techniques in conjunction with linear regression to mitigate such errors.