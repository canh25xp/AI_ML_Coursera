WEBVTT

1
00:00:00.000 --> 00:00:10.800
Welcome to Regression Metrics and Evaluation Techniques.

2
00:00:10.800 --> 00:00:14.539
After watching this video, you will be able to explain the need to evaluate regression

3
00:00:14.539 --> 00:00:19.520
models, define the error of a model, and describe certain regression metrics and evaluation

4
00:00:19.520 --> 00:00:20.520
techniques.

5
00:00:20.520 --> 00:00:23.680
You will also be able to compare different regression metrics.

6
00:00:23.680 --> 00:00:26.719
Did you know that regression models are not foolproof?

7
00:00:26.719 --> 00:00:28.220
They often make prediction errors.

8
00:00:28.220 --> 00:00:33.299
Evaluating a regression model involves determining how accurately the model can predict continuous

9
00:00:33.299 --> 00:00:36.840
numerical values, such as exam grades.

10
00:00:36.840 --> 00:00:40.400
Consider a scenario where you want to predict your grades for your final exam after receiving

11
00:00:40.400 --> 00:00:41.979
your midterm scores.

12
00:00:41.979 --> 00:00:43.639
You have fit your regression line.

13
00:00:43.639 --> 00:00:46.000
The blue dots are the grades you've received.

14
00:00:46.000 --> 00:00:49.900
The differences between the line and the blue dots are called errors.

15
00:00:49.900 --> 00:00:53.040
The predicted values minus the actual values.

16
00:00:53.040 --> 00:00:56.959
In the context of linear regression, the error of the model is a measure of the difference

17
00:00:56.959 --> 00:01:00.560
between the data points and the trend line generated by the algorithm.

18
00:01:00.560 --> 00:01:05.400
Since there are multiple data points, an error can be determined in various ways.

19
00:01:05.400 --> 00:01:10.319
Regression metrics provide insight into a model's performance, such as its accuracy,

20
00:01:10.319 --> 00:01:12.800
error distribution, and error magnitude.

21
00:01:12.800 --> 00:01:15.959
There are four metrics that are most important in regression.

22
00:01:15.959 --> 00:01:21.120
MAE, or Mean Absolute Error, is the average absolute difference between the values fitted

23
00:01:21.120 --> 00:01:24.319
by the model and the observed historical data.

24
00:01:24.400 --> 00:01:29.440
MSE, or Mean Squared Error, is the sum of the squared difference between the values

25
00:01:29.440 --> 00:01:34.680
fitted by the model and observed values divided by the number of historical points minus the

26
00:01:34.680 --> 00:01:37.239
number of parameters in the model.

27
00:01:37.239 --> 00:01:43.080
RMSE, or Root Mean Squared Error, is the square root of the MSE.

28
00:01:43.080 --> 00:01:48.239
This is a popular evaluation metric because it has the same units as the target variable,

29
00:01:48.239 --> 00:01:50.919
making it easier to interpret than MSE.

30
00:01:50.919 --> 00:01:55.199
R-squared is the amount of variance in the dependent variable that the independent variable

31
00:01:55.199 --> 00:01:56.400
can explain.

32
00:01:56.400 --> 00:02:01.720
It is also called the coefficient of determination and measures the model's goodness of fit.

33
00:02:01.720 --> 00:02:08.039
The values range from 0 to 1, with 0 being a badly fit model and 1 being a perfect model.

34
00:02:08.039 --> 00:02:12.800
Values between 0 and 1 are what you can expect from real-world scenarios as there is no perfect

35
00:02:12.800 --> 00:02:15.160
model in real life.

36
00:02:15.160 --> 00:02:20.440
Consider these evaluation metrics, which compare a model's predicted values, y i-hat, with

37
00:02:20.440 --> 00:02:23.039
the actual values, y-l.

38
00:02:23.039 --> 00:02:27.800
No single evaluation metric can give you a universally good measure of goodness of fit.

39
00:02:27.800 --> 00:02:30.000
You need to visualize your results.

40
00:02:30.000 --> 00:02:33.699
Plot the actual and predicted values for your model to see how well your regression model

41
00:02:33.699 --> 00:02:35.720
fits the labels.

42
00:02:35.720 --> 00:02:39.960
Explained variance is the sum of squared differences between the predictions and the average value

43
00:02:39.960 --> 00:02:42.039
of the actual target data.

44
00:02:42.039 --> 00:02:47.199
For a perfect predictor, explained variance is the same as the total variance of the target,

45
00:02:47.199 --> 00:02:52.199
where total variance, usually denoted as sigma squared, measures the total variable

46
00:02:52.199 --> 00:02:55.039
of the target values from their mean value.

47
00:02:55.039 --> 00:02:59.479
R squared is a popular evaluation metric that measures the proportion of variance in the

48
00:02:59.479 --> 00:03:03.559
target variable that is predictable from the input variables.

49
00:03:03.559 --> 00:03:08.039
R squared is a simple measure for describing model performance, claiming that the model

50
00:03:08.039 --> 00:03:14.139
explains 85% of the variation in the outcome is readily understood by non-technical people.

51
00:03:14.139 --> 00:03:18.419
It is important to remember that R squared assumes the target is linearly related to

52
00:03:18.419 --> 00:03:19.940
the input features.

53
00:03:19.940 --> 00:03:22.339
It can be misleading for nonlinear models.

54
00:03:22.339 --> 00:03:29.220
If the model perfectly predicts all data points, in that case, y i-hat equals y i for all i,

55
00:03:29.220 --> 00:03:33.240
the explained variance is identical to the total variance, and the unexplained variance

56
00:03:33.240 --> 00:03:34.240
is zero.

57
00:03:34.240 --> 00:03:38.139
Thus, R squared is one for a perfect model.

58
00:03:38.139 --> 00:03:42.259
Suppose the model's predictions are extremely simplistic, constantly predicting the mean

59
00:03:42.259 --> 00:03:44.320
of the data for every data point.

60
00:03:44.320 --> 00:03:49.419
In that case, it doesn't explain any variance, because the explained variance is zero, and

61
00:03:49.419 --> 00:03:53.020
the total variance is then precisely the unexplained variance.

62
00:03:53.020 --> 00:03:58.259
Thus, the R squared is zero for the simplistic mean-value model.

63
00:03:58.259 --> 00:04:03.419
Negative R squared values mean the model performs so poorly that unexplained variance exceeds

64
00:04:03.419 --> 00:04:05.419
total variance.

65
00:04:05.419 --> 00:04:09.860
Three linear regression results are shown here based on a simulated target variable

66
00:04:10.020 --> 00:04:14.860
with an exponential distribution, commonly known as log-normal distribution.

67
00:04:14.860 --> 00:04:19.899
The linear model fits three versions of the target variable, the original target, a Box-Cox

68
00:04:19.899 --> 00:04:24.700
transformed version of the target, and a logarithmically transformed version.

69
00:04:24.700 --> 00:04:29.220
Visually, you can see that the models progressively fit the target data better, and better because

70
00:04:29.220 --> 00:04:34.459
the data becomes more concentrated around the best-fit line with each transformation.

71
00:04:34.459 --> 00:04:38.779
The various evaluation metrics displayed for each of the three models fit to the transformed

72
00:04:38.820 --> 00:04:43.100
targets are all consistent with our visual intuition of the result.

73
00:04:43.100 --> 00:04:48.500
R squared increased significantly for Box-Cox and log transformations, and the error metrics

74
00:04:48.500 --> 00:04:53.980
MAE, MSE, and RMSE all improved substantially.

75
00:04:53.980 --> 00:04:58.059
In this video, you learned that evaluating a regression model involves determining how

76
00:04:58.059 --> 00:05:01.700
accurately the model can predict continuous numerical values.

77
00:05:01.700 --> 00:05:05.140
The error of the model is a measure of the difference between the data points and the

78
00:05:05.140 --> 00:05:07.619
trend line generated by the algorithm.

79
00:05:07.619 --> 00:05:14.420
The essential regression metrics are MAE, MSE, RMSE, and R squared.

80
00:05:14.420 --> 00:05:18.579
Explained variance is the sum of squared differences between the predictions and the average value

81
00:05:18.579 --> 00:05:20.339
of the actual target data.

82
00:05:20.339 --> 00:05:24.420
R squared measures the proportion of variance in the target variable that is predictable

83
00:05:24.420 --> 00:05:25.779
from the input variables.