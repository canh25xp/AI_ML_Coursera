WEBVTT

1
00:00:00.000 --> 00:00:11.800
Welcome to Classification Metrics and Evaluation Techniques.

2
00:00:11.800 --> 00:00:16.520
After watching this video, you will be able to define the train-test-split technique,

3
00:00:16.520 --> 00:00:22.879
describe confusion matrix, accuracy, precision, recall, and F1 score metrics, and illustrate

4
00:00:22.879 --> 00:00:24.360
examples of each.

5
00:00:24.360 --> 00:00:28.239
Let's begin by understanding what supervised learning evaluation is.

6
00:00:28.239 --> 00:00:32.680
Supervised learning evaluation establishes how well a machine learning model can predict

7
00:00:32.680 --> 00:00:34.200
the outcome for unseen data.

8
00:00:34.200 --> 00:00:39.060
It is essential for understanding model effectiveness and involves comparing model predictions to

9
00:00:39.060 --> 00:00:40.919
ground truth labels.

10
00:00:40.919 --> 00:00:47.319
During training, the model tries to optimize predictions based on one or more evaluation metrics.

11
00:00:47.319 --> 00:00:52.200
After training, the model is again evaluated to estimate how well it can generalize to unseen data.

12
00:00:52.200 --> 00:00:57.880
Supervised learning evaluation is essential in both the training and testing phases.

13
00:00:57.880 --> 00:01:01.080
When you're training a machine learning model to predict an outcome, you don't want

14
00:01:01.080 --> 00:01:03.759
to feed all the data from the dataset to the model.

15
00:01:03.759 --> 00:01:07.680
The train-test-split technique is used to estimate the performance of machine learning

16
00:01:07.680 --> 00:01:11.080
algorithms when they're used to make predictions.

17
00:01:11.080 --> 00:01:15.680
In the train-test-split technique, the dataset is split into two parts – the training set

18
00:01:15.680 --> 00:01:17.000
and the test set.

19
00:01:17.000 --> 00:01:21.040
The training subset comprises around 70 to 80 percent of the data and is used to train

20
00:01:21.040 --> 00:01:22.239
the model.

21
00:01:22.239 --> 00:01:27.839
The test subset is used to evaluate how well the model generalizes to new unseen data.

22
00:01:27.839 --> 00:01:32.279
In classification tasks, the model predicts categorical labels to assess how well these

23
00:01:32.279 --> 00:01:34.400
predictions align with the actual labels.

24
00:01:34.400 --> 00:01:38.760
We'll explore some common metrics for evaluating classification models.

25
00:01:38.760 --> 00:01:42.879
Accuracy is the ratio of correctly predicted instances to the total number of instances

26
00:01:42.879 --> 00:01:43.879
in the dataset.

27
00:01:43.879 --> 00:01:48.599
A confusion matrix is a table that breaks down the number of ground truth instances

28
00:01:48.599 --> 00:01:52.800
of a specific class against the number of predicted class instances.

29
00:01:52.800 --> 00:01:57.400
Precision measures how many of the predicted positive instances are actually positive.

30
00:01:57.639 --> 00:02:02.000
Recall measures how many of the actual positive instances are correctly predicted.

31
00:02:02.000 --> 00:02:07.319
F1 score combines precision and recall to represent a model's accuracy.

32
00:02:07.319 --> 00:02:13.039
To calculate accuracy, consider a will-I-pass-or-fail-my-biology-test example.

33
00:02:13.039 --> 00:02:17.479
Assume your model has been trained and has some predictions on the test set.

34
00:02:17.479 --> 00:02:21.279
You represent pass with green squares and fail with red.

35
00:02:21.279 --> 00:02:25.479
You can calculate accuracy by taking the number of correctly classified observations and dividing

36
00:02:25.479 --> 00:02:27.520
it by the number of observations.

37
00:02:27.520 --> 00:02:29.880
The misclassified points are highlighted in grey.

38
00:02:29.880 --> 00:02:32.440
That'll give you 70%.

39
00:02:32.440 --> 00:02:37.399
Displayed here is a confusion matrix commonly used for evaluating classification performance.

40
00:02:37.399 --> 00:02:42.960
On the y-axis, you have the true labels, and on the x-axis, you have the predicted labels.

41
00:02:42.960 --> 00:02:47.800
The numbers in the boxes are the counts of true positives, true negatives, false positives,

42
00:02:47.800 --> 00:02:49.279
and false negatives.

43
00:02:49.279 --> 00:02:53.520
True positive means you predicted pass, and it was pass.

44
00:02:53.520 --> 00:02:56.720
True negative means you predicted fail, and it was fail.

45
00:02:56.720 --> 00:03:00.320
False positives mean you predicted pass, but it is actually fail.

46
00:03:00.320 --> 00:03:04.839
False negative means you predicted fail, and it is actually pass.

47
00:03:04.839 --> 00:03:09.119
Shown here is the decision boundary for a KNN classifier trained to predict iris types

48
00:03:09.119 --> 00:03:12.600
based on the iris flower dataset in Scikit-learn.

49
00:03:12.600 --> 00:03:14.399
Accuracy is high at 93%.

50
00:03:14.399 --> 00:03:18.119
As you can see from the background colors, which distinguish the three prediction types,

51
00:03:18.119 --> 00:03:22.279
as compared with the dots colored by their actual types, there are very few misclassified

52
00:03:22.279 --> 00:03:23.279
colors.

53
00:03:23.279 --> 00:03:28.960
Here, you see a heat map displaying the confusion matrix for the KNN classifier, colored on

54
00:03:28.960 --> 00:03:33.360
a scale ranging from purple at the lowest and yellow at the highest values.

55
00:03:33.360 --> 00:03:36.960
The colors represent the number of predictions made for each class that fall within each

56
00:03:36.960 --> 00:03:38.399
actual class.

57
00:03:38.399 --> 00:03:43.559
For example, for predictions classified as setosa, the entries show the counts of how

58
00:03:43.559 --> 00:03:48.600
many of those predictions should have been setosa, versicolor, or virginica.

59
00:03:48.600 --> 00:03:52.059
The diagonal entries are the predictions the classifier got right.

60
00:03:52.059 --> 00:03:55.419
The diagonal is hot in this case, which is good.

61
00:03:55.419 --> 00:03:59.899
When evaluating a classification model, a data scientist also considers other metrics.

62
00:03:59.899 --> 00:04:02.679
Let's look at the pass or fail example.

63
00:04:02.679 --> 00:04:06.860
In the pass class, precision is the fraction of true positives among all the examples that

64
00:04:06.860 --> 00:04:09.220
were predicted to be positives.

65
00:04:09.220 --> 00:04:13.259
Precision is the number of true positives divided by the number of positive predictions.

66
00:04:13.259 --> 00:04:17.940
An example where precision may be more important than accuracy is a movie recommendation engine

67
00:04:17.940 --> 00:04:21.160
where it may cost more to promote a certain movie to a user.

68
00:04:21.160 --> 00:04:24.519
If the movie was a false positive, meaning that the user isn't interested in the movie

69
00:04:24.519 --> 00:04:29.000
that was recommended, then that would be an additional cost with no benefit.

70
00:04:29.000 --> 00:04:31.239
Now let's take a look at recall.

71
00:04:31.239 --> 00:04:35.940
Recall is the fraction of true positives among all the examples that were actually positive.

72
00:04:35.940 --> 00:04:39.739
Consider the number of pass observations the model got right out of the total true pass

73
00:04:39.739 --> 00:04:41.440
observations.

74
00:04:41.440 --> 00:04:44.239
That is 4 out of 7, or 57.1%.

75
00:04:44.239 --> 00:04:50.540
It is the number of true positives divided by the sum of true positives and false negatives.

76
00:04:50.540 --> 00:04:55.260
When opportunity cost is more important, recall may be a more important metric.

77
00:04:55.260 --> 00:04:57.500
An example of this is in the medical field.

78
00:04:57.500 --> 00:05:01.660
It's important to account for false negatives, especially regarding patient health.

79
00:05:01.660 --> 00:05:04.739
Finally, let's look at the F1 score.

80
00:05:04.739 --> 00:05:07.940
Imagine that you are in the medical field and have incorrectly classified patients as

81
00:05:07.940 --> 00:05:09.299
having an illness.

82
00:05:09.299 --> 00:05:11.700
You could be treating the wrong diagnosis.

83
00:05:11.700 --> 00:05:16.019
In cases such as this, where precision and recall are equally important, you can't try

84
00:05:16.019 --> 00:05:17.899
to optimize one or the other.

85
00:05:17.899 --> 00:05:22.739
The F1 score, which is defined as the harmonic or balanced mean of precision and recall,

86
00:05:22.739 --> 00:05:23.739
is useful.

87
00:05:23.739 --> 00:05:30.700
It is calculated as 2 multiplied by precision and recall divided by precision plus recall.

88
00:05:30.700 --> 00:05:35.420
Here is a table summarizing the precision, recall, and F1 scores for each of the actual

89
00:05:35.420 --> 00:05:39.980
classes – setosa, versicolor, and virginica.

90
00:05:39.980 --> 00:05:43.179
Setosa prediction scored perfectly at 1 for each metric.

91
00:05:43.179 --> 00:05:47.040
The weighted average of the metrics is weighted by the support of each class, or number of

92
00:05:47.040 --> 00:05:49.260
flowers in each class.

93
00:05:49.260 --> 00:05:53.040
In this video, you learned that supervised learning evaluation establishes how well

94
00:05:53.040 --> 00:05:56.839
a machine learning model can predict the outcome for unseen data.

95
00:05:56.839 --> 00:06:00.320
The train-test-split technique is used to estimate the prediction performance of machine

96
00:06:00.320 --> 00:06:02.920
learning algorithms for unseen data.

97
00:06:02.920 --> 00:06:09.000
Common metrics for evaluating classification models include accuracy, confusion matrix,

98
00:06:09.000 --> 00:06:11.220
precision, and recall.

99
00:06:11.220 --> 00:06:16.140
The F1 score is the harmonic or balanced mean of precision and recall.