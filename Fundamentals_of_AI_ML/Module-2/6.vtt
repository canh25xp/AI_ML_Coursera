WEBVTT

1
00:00:05.000 --> 00:00:11.199
Welcome to Introduction to Polynomial and Nonlinear Regression.

2
00:00:11.199 --> 00:00:13.520
After watching this video, you will be able to

3
00:00:13.520 --> 00:00:17.680
Describe polynomial regression, describe nonlinear regression,

4
00:00:17.680 --> 00:00:21.920
and demonstrate applications of nonlinear regression.

5
00:00:21.920 --> 00:00:25.799
Nonlinear regression is a statistical method for
modeling the relationship between a dependent

6
00:00:25.799 --> 00:00:30.280
variable and one or more independent variables,
where the relationship is represented by a

7
00:00:30.280 --> 00:00:32.159
nonlinear equation.

8
00:00:32.159 --> 00:00:37.880
This equation could be polynomial, exponential,
logarithmic, or any other function that does

9
00:00:37.880 --> 00:00:41.159
not use linear parameters.

10
00:00:41.159 --> 00:00:44.639
Nonlinear regression is useful when there is a
complex relationship between variables

11
00:00:44.639 --> 00:00:46.919
that cannot be captured through a straight line.

12
00:00:46.919 --> 00:00:50.959
For instance, you would use nonlinear regression
when you are using a dataset that follows

13
00:00:50.959 --> 00:00:52.959
an exponential growth pattern.

14
00:00:52.959 --> 00:00:54.880
Let's consider the two charts here.

15
00:00:54.880 --> 00:00:58.000
The red line through the data in the left chart
is a reasonable fit.

16
00:00:58.000 --> 00:01:02.319
However, with real-world data, the relationship
between your input and target variables is

17
00:01:02.319 --> 00:01:03.560
rarely linear.

18
00:01:03.560 --> 00:01:07.760
More commonly, your data has a background
trend that follows a smoothed curve rather

19
00:01:07.760 --> 00:01:09.040
than a straight line.

20
00:01:09.040 --> 00:01:13.319
Like in the right chart, clearly a smooth
nonlinear curve does a better job at approximating

21
00:01:13.319 --> 00:01:15.099
data than the straight line.

22
00:01:15.099 --> 00:01:17.720
The straight line underfits the data.

23
00:01:17.720 --> 00:01:22.400
You can use many kinds of nonlinear
regression methods to model your dataset.

24
00:01:22.400 --> 00:01:26.120
Polynomial regression uses an ordinary
linear regression to indirectly fit your data to

25
00:01:26.120 --> 00:01:30.760
polynomial expressions of the features,
rather than the features themselves.

26
00:01:30.760 --> 00:01:34.839
Nonlinear regression follows the same idea,
but bases its inputs on functions of the given

27
00:01:34.839 --> 00:01:39.440
features, such as the logarithm or exponential of the features.

28
00:01:39.440 --> 00:01:42.839
Nonlinear regression doesn't necessarily
reduce to linear regression like polynomial

29
00:01:42.839 --> 00:01:44.319
regression does.

30
00:01:44.319 --> 00:01:49.199
In this chart, you can see linear, quadratic,
and cubic regression curves that fit the patterns

31
00:01:49.239 --> 00:01:50.800
in the data quite well.

32
00:01:50.800 --> 00:01:55.279
And this approach can go on and on to polynomials of arbitrary degree.

33
00:01:55.279 --> 00:01:59.879
We can call all of these polynomial regression,
where the relationship between the independent

34
00:01:59.879 --> 00:02:05.720
variable x and the dependent variable y is
modeled as an nth degree polynomial in x.

35
00:02:05.720 --> 00:02:10.320
Consider this polynomial regression example
where a good candidate for a fit is a cubic

36
00:02:10.320 --> 00:02:13.160
or third degree polynomial, given by

37
00:02:13.160 --> 00:02:20.000
y equals theta zero plus theta one x plus
theta two x squared plus theta three x cubed.

38
00:02:20.000 --> 00:02:25.119
Here, the thetas are parameters to be estimated
that best fit the underlying data, by introducing

39
00:02:25.119 --> 00:02:30.559
new variables as x underscore one equals x,
x underscore two equals x to the power of

40
00:02:30.559 --> 00:02:34.600
two, and x underscore three equals x to the power of three.

41
00:02:34.600 --> 00:02:38.399
The model can now be expressed as a linear
combination of the new variables as

42
00:02:38.399 --> 00:02:44.880
y equals theta zero plus theta one x one plus
theta two x two plus theta three x three.

43
00:02:44.880 --> 00:02:49.399
Because the resulting model has been linearized,
you can simply use ordinary multiple linear

44
00:02:49.399 --> 00:02:52.520
regression to find the best fit parameters.

45
00:02:52.520 --> 00:02:57.259
Given any finite sets of points, it's always
possible to find a polynomial of sufficiently

46
00:02:57.259 --> 00:03:00.520
high degree that will pass through every point.

47
00:03:00.520 --> 00:03:03.720
Such a perfect fit amounts to overfitting as seen in this chart.

48
00:03:03.720 --> 00:03:08.039
The polynomial regression model memorizes
everything including any random noise or

49
00:03:08.039 --> 00:03:12.279
large variations, rather than understanding the underlying patterns.

50
00:03:12.279 --> 00:03:16.080
It's important to pick a regression that
fits the data well without overfitting.

51
00:03:16.080 --> 00:03:20.000
You don't need to capture every fine detail,
just the trend.

52
00:03:20.000 --> 00:03:22.800
Polynomial regression is a special form of nonlinear regression.

53
00:03:22.800 --> 00:03:27.800
It expresses a nonlinear dependence on the
input features, but it has a linear dependence

54
00:03:27.800 --> 00:03:32.039
on the regression coefficients because it
can be transformed into a linear regression

55
00:03:32.039 --> 00:03:33.039
problem.

56
00:03:33.039 --> 00:03:35.839
It is often simply called linear regression.

57
00:03:35.839 --> 00:03:40.800
In contrast, there are many real-world complex
nonlinear relationships that can't be modeled

58
00:03:40.800 --> 00:03:42.399
as polynomials.

59
00:03:42.399 --> 00:03:47.380
Such common examples of nonlinear regression
include exponential or compound growth.

60
00:03:47.380 --> 00:03:51.600
For example, how investments grow with compound interest rates.

61
00:03:51.600 --> 00:03:52.880
Logarithmic.

62
00:03:52.880 --> 00:03:57.880
For example, law of diminishing returns,
how incremental gains in productivity or profit

63
00:03:57.880 --> 00:04:02.800
can reduce as investment in a production factor,
such as labor increases.

64
00:04:02.800 --> 00:04:04.080
Periodicity.

65
00:04:04.080 --> 00:04:10.720
For example, sinusoidal seasonal variations
in a quantity, such as monthly rainfall or temperature.

66
00:04:10.720 --> 00:04:15.520
Let's consider this data corresponding to
China's Gross Domestic Product, or GDP,

67
00:04:15.520 --> 00:04:17.760
from 1960 to 2014.

68
00:04:17.760 --> 00:04:22.359
Each row provides China's annual GDP in US dollars for the year.

69
00:04:22.359 --> 00:04:27.839
The scatter plot displays a strong dependence
of GDP on time, but the relationship is nonlinear.

70
00:04:27.839 --> 00:04:32.820
As you can see, GDP increases over time,
and the rate of this growth also increases.

71
00:04:32.820 --> 00:04:36.100
This increasing growth rate is characteristic of exponential growth.

72
00:04:36.100 --> 00:04:41.899
A reasonable regression model then uses an
exponential function, like y-hat equals theta-zero

73
00:04:41.899 --> 00:04:45.019
plus theta-one, e-x.

74
00:04:45.019 --> 00:04:49.260
Consider the following simulated example involving
human productivity as a function of the number

75
00:04:49.260 --> 00:04:51.420
of consecutive hours worked.

76
00:04:51.420 --> 00:04:54.420
Working more hours per day, on average,
increases your productivity.

77
00:04:54.420 --> 00:04:59.299
However, after a reasonable limit, say 6 hours of work,
each additional work hour generates

78
00:04:59.299 --> 00:05:02.660
less productivity per hour than the previous hour.

79
00:05:02.660 --> 00:05:05.179
This is an example of diminishing returns.

80
00:05:05.179 --> 00:05:10.720
The first 6 hours of the model show a linear
increase in cumulative productivity, but then,

81
00:05:10.720 --> 00:05:13.899
the returns slow down and become logarithmic.

82
00:05:13.899 --> 00:05:17.579
There are many methods to determine what kind of regression model you need.

83
00:05:17.579 --> 00:05:21.940
One technique is to visually determine
whether the relation is linear or nonlinear.

84
00:05:21.940 --> 00:05:26.779
Analyzing scatter plots of your target variable
against each input variable can reveal patterns

85
00:05:26.779 --> 00:05:28.380
in the dependencies.

86
00:05:28.380 --> 00:05:32.820
Try to express these patterns as mathematical
functions and determine if they're linear,

87
00:05:32.820 --> 00:05:36.420
exponential, logarithmic, or sinusoidal.

88
00:05:36.420 --> 00:05:38.899
Generate models and analyze your results.

89
00:05:38.899 --> 00:05:42.820
There is always a chance that your data might
have no relationship with your target.

90
00:05:42.820 --> 00:05:46.299
You can visually interpret your model's
errors by plotting its predictions against

91
00:05:46.299 --> 00:05:48.679
the actual target values.

92
00:05:48.679 --> 00:05:51.459
How can you find an optimal nonlinear model?

93
00:05:51.459 --> 00:05:55.320
If you have a mathematical expression for your
proposed model, you can use an optimization

94
00:05:55.320 --> 00:05:58.239
technique like gradient descent to find optimal parameters.

95
00:05:58.239 --> 00:06:03.079
Otherwise, if you haven't decided on a specific
regression model, you can select amongst many

96
00:06:03.079 --> 00:06:04.880
machine learning models.

97
00:06:04.880 --> 00:06:10.959
Some options include regression trees, random forests,
neural networks, support vector machines,

98
00:06:10.959 --> 00:06:14.480
gradient boosting machines, k-nearest neighbors.

99
00:06:14.480 --> 00:06:19.880
Nonlinear regression uses polynomial, exponential,
logarithmic equations to model data.

100
00:06:19.880 --> 00:06:24.220
It is used when the relationship between variables
cannot be captured through a straight line.

101
00:06:24.220 --> 00:06:28.160
In this video, you learned how you can use
polynomial regression to fit your data to

102
00:06:28.160 --> 00:06:30.540
polynomial expressions of the features.

103
00:06:30.540 --> 00:06:35.179
The polynomial regression model memorizes everything,
including any random noise or

104
00:06:35.179 --> 00:06:39.339
large variations, rather than understanding the underlying patterns.

105
00:06:39.339 --> 00:06:44.720
There are many real-world complex nonlinear
relationships that can't be modeled as polynomials.

106
00:06:44.720 --> 00:06:50.459
Some common examples of nonlinear regression
include exponential or compound growth, logarithmic,

107
00:06:50.459 --> 00:06:51.700
and periodicity.

108
00:06:51.700 --> 00:06:54.820
There are many methods to determine what
kind of regression model you need.

109
00:06:54.820 --> 00:06:58.679
You can analyze scatter plots of your target
variable against each input variable to reveal

110
00:06:58.679 --> 00:07:00.579
patterns in the dependencies.

111
00:07:00.579 --> 00:07:04.859
To find an optimal nonlinear model, you can
select amongst many machine learning models

112
00:07:04.859 --> 00:07:08.459
such as regression trees, random forests,
and k-nearest neighbors.