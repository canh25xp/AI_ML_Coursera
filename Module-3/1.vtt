WEBVTT

1
00:00:00.000 --> 00:00:10.039
Welcome to this video on classification.

2
00:00:10.039 --> 00:00:12.000
After watching this video, you will be able to:

3
00:00:12.000 --> 00:00:18.200
describe the classification method of supervised learning, discuss the applications and use cases of classification,

4
00:00:18.200 --> 00:00:24.060
list the different classification algorithms, and explain how to make multi-class predictions.

5
00:00:24.060 --> 00:00:27.239
Classification is a supervised machine learning, or ML method,

6
00:00:27.239 --> 00:00:31.479
that uses fully trained models to predict labels on new data.

7
00:00:31.479 --> 00:00:36.239
The labels in classification form a categorical variable with discrete values.

8
00:00:36.239 --> 00:00:41.360
Supervised learning aims to understand data in the correct context when answering a specific question.

9
00:00:41.360 --> 00:00:44.139
This ensures data accuracy when making predictions.

10
00:00:44.139 --> 00:00:48.040
As data is input into the model, the model adjusts the data to fit the algorithm

11
00:00:48.040 --> 00:00:52.340
and classifies it accordingly, defining the input and the predicted output.

12
00:00:52.340 --> 00:00:56.479
Classification has several applications in a wide variety of industries.

13
00:00:56.479 --> 00:01:00.700
Many problems can be expressed as associations between feature and target variables,

14
00:01:00.700 --> 00:01:03.340
particularly when labeled data is available.

15
00:01:03.340 --> 00:01:08.000
Classification can be used to build applications for email filtering, speech-to-text,

16
00:01:08.000 --> 00:01:13.720
handwriting recognition, biometric identification, document classification, and much more.

17
00:01:13.720 --> 00:01:17.599
Churn prediction is when you use machine learning classification to predict whether a customer

18
00:01:17.599 --> 00:01:19.760
will discontinue a service.

19
00:01:19.760 --> 00:01:24.639
Customer segmentation is when you use classification to predict the category to which a customer belongs.

20
00:01:24.639 --> 00:01:30.040
You can also use classification to predict whether a customer will likely respond to an advertising campaign.

21
00:01:30.040 --> 00:01:33.080
Let's explore additional use cases of classification.

22
00:01:33.080 --> 00:01:38.000
Suppose a bank is concerned that some of their loan applicants may be unable to repay their loan.

23
00:01:38.000 --> 00:01:42.720
The bank uses historical loan default data to predict which customers are likely to default.

24
00:01:42.720 --> 00:01:48.560
Here, a classifier can be trained to use customer information like age, income, and credit debt levels

25
00:01:48.560 --> 00:01:51.080
to learn whether they will default.

26
00:01:51.080 --> 00:01:53.080
Given a new customer and the same information,

27
00:01:53.080 --> 00:01:55.740
but without the knowledge of the likelihood of defaulting,

28
00:01:55.740 --> 00:02:00.760
the trained classification model predicts whether the customer is likely to default.

29
00:02:00.760 --> 00:02:06.559
This is an example of a binary classifier, as its predictions are limited to two possible classes.

30
00:02:06.559 --> 00:02:12.160
Now, consider the following example of a multi-class classifier used to help prescribe various drugs.

31
00:02:12.160 --> 00:02:16.940
Imagine that you collected data about a set of patients, all of whom suffered from the same illness.

32
00:02:16.940 --> 00:02:21.820
During their course of treatment, each patient responded positively to one of three medications.

33
00:02:21.820 --> 00:02:26.699
You can use this labeled dataset with a classification algorithm to build a classification model

34
00:02:26.699 --> 00:02:31.220
that can predict which drug might be appropriate for a future patient with the same illness.

35
00:02:31.220 --> 00:02:35.300
You can use many different types of algorithms to build your classification model.

36
00:02:35.300 --> 00:02:40.979
Some common machine learning classification algorithms include Naive Bayes, Logistic Regression,

37
00:02:40.979 --> 00:02:47.100
Decision Trees, K-Nearest Neighbors, Support Vector Machines, and Neural Networks.

38
00:02:47.100 --> 00:02:53.419
Algorithms like Logistic Regression, KNN, and Decision Trees can learn how to distinguish multiple classes.

39
00:02:53.419 --> 00:02:57.420
Many classification algorithms are not able to make distinctions between more than two classes,

40
00:02:57.420 --> 00:03:02.259
but you can use them as components for multi-class classifiers.

41
00:03:02.259 --> 00:03:05.660
Strategies for extending binary classifiers to handle multiple classes

42
00:03:05.660 --> 00:03:10.179
include one-versus-all classification and one-versus-one classification.

43
00:03:10.179 --> 00:03:14.139
The one-versus-all scheme implements a set of independent binary classifiers,

44
00:03:14.139 --> 00:03:16.779
one for each class label in the dataset.

45
00:03:16.779 --> 00:03:20.979
Each classifier is assigned a single label that defines its target class.

46
00:03:20.979 --> 00:03:24.940
Each classifier's task is to make a binary prediction for every data point about whether

47
00:03:24.940 --> 00:03:28.740
it has the given label, a one-versus-the-rest classifier.

48
00:03:28.740 --> 00:03:33.699
As you can see, if there are k classes, there will be exactly k binary classifiers contributing.

49
00:03:33.699 --> 00:03:38.800
Let's understand how the one-versus-all strategy decomposes a set of data points.

50
00:03:38.800 --> 00:03:41.919
The algorithm works on each class label, one at a time,

51
00:03:41.919 --> 00:03:46.639
with a trained outcome showing the points predicted to have a given color.

52
00:03:46.639 --> 00:03:50.059
Notice also that a given data point might not belong to any of the classes,

53
00:03:50.059 --> 00:03:54.160
as it might not get picked up by any of the individual classifiers.

54
00:03:54.160 --> 00:03:57.500
Such unclassified points fall into another class.

55
00:03:57.500 --> 00:04:01.479
This property might be useful for identifying outliers or noise.

56
00:04:01.479 --> 00:04:05.990
With the one-versus-one strategy, instead of each classifier deciding whether a point belongs to a class,

57
00:04:05.990 --> 00:04:12.240
the question changes from "Is it this?" to "Is it this or is it that?"

58
00:04:12.240 --> 00:04:15.979
Given a set of classes, consider all possible pairs of classes.

59
00:04:15.979 --> 00:04:20.480
For each pair of labels, a classifier is trained on the subset of the data corresponding to

60
00:04:20.480 --> 00:04:24.540
the two labels and decides which class each point belongs to.

61
00:04:24.540 --> 00:04:27.799
The process continues until all classifiers are trained.

62
00:04:27.799 --> 00:04:31.799
The final class label assigned to each point may be decided by a voting scheme.

63
00:04:31.799 --> 00:04:33.880
The simplest scheme is by popularity,

64
00:04:33.880 --> 00:04:37.880
meaning the class predicted by the most binary classifiers wins.

65
00:04:37.880 --> 00:04:39.619
Here, green is the winner.

66
00:04:39.619 --> 00:04:40.880
What if there is a tie?

67
00:04:40.880 --> 00:04:44.239
Here, we have three classes with the same number of votes.

68
00:04:44.239 --> 00:04:48.339
In a scenario where that is possible, it would be better to use an improved scheme,

69
00:04:48.339 --> 00:04:53.920
weighing each vote by the confidence level or probability assigned to that class for each classifier.

70
00:04:53.920 --> 00:04:58.299
Alternatively, you could try using one-versus-all classification instead.

71
00:04:58.299 --> 00:05:02.220
In this video, you learned that classification is a supervised ML method

72
00:05:02.220 --> 00:05:06.360
that uses fully trained models to predict labels on new data.

73
00:05:06.360 --> 00:05:10.019
Classification can be used for churn prediction, customer segmentation,

74
00:05:10.019 --> 00:05:13.100
and predicting advertising campaign responsiveness.

75
00:05:13.100 --> 00:05:18.260
Use cases of classification also include loan default prediction and multi-class drug prescription.

76
00:05:18.260 --> 00:05:23.239
Classification has several algorithms, which also include multi-class classifiers.

77
00:05:23.239 --> 00:05:28.339
Binary classifiers can be extended to handle multiple classes by using certain strategies.

78
00:05:28.339 --> 00:05:33.380
The one-versus-all scheme implements independent binary classifiers, one for each class label.

79
00:05:33.380 --> 00:05:37.299
The one-versus-one strategy answers the question, "Is it this or is it that?"