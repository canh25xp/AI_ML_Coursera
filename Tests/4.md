# Coursera - Machine Learning with Python - 4th test

Q1. In logistic regression given the input $x$, and parameters $w \in \mathbb{R}^{n_x}$, $b \in \mathbb{R}$,
how do we generate the output $\hat{y}$ ?

1. $\sigma(Wx)$
2. $Wx + b$
3. $\sigma(Wx + b)$
4. $\tanh(Wx + b)$

Ans: 3. $\sigma(Wx + b)$

In logistic regression, the model computes:

$$
z = w^\top x + b
$$

Then passes it through the **sigmoid function** to produce a probability:

$$
\hat{y} = \sigma(z) = \sigma(w^\top x + b)
$$

This ensures the output is between **0 and 1**, appropriate for binary classification.

---

Q2. Which of these is the "Logistic Loss"

1. $L^i (\hat{y}^i, y^i) = | y^i - \hat{y}^i |^2$
2. $L^i (\hat{y}^i, y^i) = -[y^i \log(\hat{y}^i) +(1-y^i)\log(1-\hat{y}^i)]$
3. $L^i (\hat{y}^i, y^i) = \max(0,y^i - \hat{y}^i$
4. $L^i (\hat{y}^i, y^i) = | y^i - \hat{y}^i |$

Ans: 2. $L^i (\hat{y}^i, y^i) = -[y^i \log(\hat{y}^i) +(1-y^i)\log(1-\hat{y}^i)]$

This is the **logistic loss**, also called **binary cross-entropy loss**, and it's used in logistic regression because:

- It penalizes confident wrong predictions heavily
- It is convex
- It works naturally with probabilities from the sigmoid function

---

Q3. Consider the `numpy` array `x`

```py
x = np.array([[[1],[2]],[[3],[4]]])
```

What is the shape of `x` ?

1. (4,)
2. (2,2,1)
3. (1,2,2)
4. (2,2)

Ans: 2. (2,2,1)

- The outermost list has **2 elements** → first dimension = 2
- Each element is a list with **2 elements** → second dimension = 2
- Each inner element is a list with **1 element** → third dimension = 1

---

Q.4 Consider the following random array `a` and `b`, and `c`:

```py
a = np.random.randn(3,3) # a.shape = (3,3)
b = np.random.randn(2,1) # b.shape = (2,1)
c = a + b
```

What will be the shape of `c`

1. (3,3)
2. (2,1)
3. (2,3,3)
4. Impossible computation

---

Q.5 Consider the two random array `a` and `b`:

```py
a = np.random.randn(1,3) # a.shape = (1,3)
b = np.random.randn(3,3) # b.shape = (3,3)
c = a * b
```

What will be the shape of `c`

1. Impossible computation because it is not possible to broadcast more than 1 dimension.
2. (1,3).
3. (3,3).
4. Impossible computation because the sizes don't match.

---

Q.6 Suppose our input batch consists of 8 greyscale images, each of dimension $8 \times 8$.
We reshape these images into feature column vectors $x$.
Remember that $X=[x^1 x^2 \dots x^8]$.
What is the dimension of $X$

1. $(512,1)$
2. $(64,8)$
3. $(8,64)$
4. $(8,8,8)$

---

Q.7 Consider the following array:

```py
a = np.array([[2,1], [1,3]])
```

What is the result of `a*a`?

1. $
\begin{pmatrix}
  5 & 5  \\
  5 & 10
\end{pmatrix}
$
2. $
\begin{pmatrix}
  4 & 1  \\
  1 & 9
\end{pmatrix}
$
3. Impossible computation
4. $
\begin{pmatrix}
  4 & 2  \\
  2 & 6
\end{pmatrix}
$

---

Q.8 Consider the following code snippet:

```py
a.shape=(4,3)
b.shape=(4,1)

for i in range(3):
  for j in range(4):
    c[i][j] = a[j][i] +b[j]
```

How do you vectorize this ?

1. `c = a.T + b`
2. `c = a + b.T`
3. `c = a.T + b.T`
4. `c = a + b`

---

Q.9 Consider the following code:

```py
a = np.random.randn(3,3)
b = np.random.randn(3,1)
c = a * b
```

What will `c` be?

---

Q.10 Consider the following computation

$$
u = a * b \\
v = a + c \\
w = b * c \\
J = u - v + w
$$

What is the output of $J$ ?
